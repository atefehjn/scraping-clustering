{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from mapping_DW import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torob = pd.read_csv('DW_models_data_torob.csv')\n",
    "digikala = pd.read_csv('DW_models_data_digikala.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'model_code', 'price', 'ارتفاع', 'سایر ویژگی‌ها', 'عمق', 'شناسه',\n",
       "       'پهنا', 'امکانات ویژه', 'سیستم ایمنی', 'تنظیمات دستگاه', 'تعداد سبد',\n",
       "       'ظرفیت (نفر)', 'وزن', 'میزان صدا', 'نوع ماشین ظرفشویی',\n",
       "       'متوسط میزان مصرف آب در هر شست و شو', 'تعداد قفسه های بالا',\n",
       "       'تعداد برنامه های شست‌وشو', 'نمودار میزان مصرف انرژی',\n",
       "       'جایگاه نگهداری'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digikala.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torob.columns\n",
    "# with open('columns.txt', 'w', encoding='utf-8') as f:\n",
    "#     for i in a:\n",
    "#         f.write(i + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "def map_and_merge_columns(df, column_mappings):\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    for new_col, old_cols in column_mappings.items():\n",
    "        matching_cols = [col for col in old_cols if col in df.columns]\n",
    "        if matching_cols:\n",
    "            if len(matching_cols) > 1:\n",
    "                df[new_col] = df[matching_cols].apply(\n",
    "                    lambda row: next((x for x in row if pd.notna(x) and x != ''), None), axis=1\n",
    "                )\n",
    "                df.drop(columns=matching_cols, inplace=True)\n",
    "            else:\n",
    "                df.rename(columns={matching_cols[0]: new_col}, inplace=True)\n",
    "\n",
    "    # Select only existing columns\n",
    "    existing_cols = [col for col in column_mappings.keys() if col in df.columns]\n",
    "    return df[existing_cols]\n",
    "\n",
    "digikala_n = map_and_merge_columns(digikala, digikala_mapping)\n",
    "torob_n = map_and_merge_columns(torob, torob_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33, 16), (87, 16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digikala_n.shape, torob_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_concate = pd.concat([digikala_n, torob_n], ignore_index=True)\n",
    "\n",
    "# Group by 'model_code' and fill missing values with available ones\n",
    "dw = dw_concate.groupby(\"model_code\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [model_code, price, height, depth, width, safety_system, number_of_baskets, capacity, weight, noise_level, dishwasher_type, average_water_consumption_per_wash, number_of_upper_racks, number_of_washing_programs, energy_rating, storage_position]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(dw[dw.duplicated(subset='model_code', keep=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "dw = dw.loc[:, dw.isnull().mean() < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/3328882249.py:7: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dw = dw.applymap(lambda x: convert_persian_to_english(str(x)) if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "def convert_persian_to_english(text):\n",
    "    persian_digits = \"۰۱۲۳۴۵۶۷۸۹\"\n",
    "    english_digits = \"0123456789\"\n",
    "    trans_table = str.maketrans(persian_digits, english_digits)\n",
    "    return text.translate(trans_table)\n",
    "# Apply the conversion function to the entire DataFrame\n",
    "dw = dw.applymap(lambda x: convert_persian_to_english(str(x)) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enery_rating_mapping = {\n",
    "    \"+++A\": 3,\n",
    "    \"++A\": 2,\n",
    "    \"A++\": 2,\n",
    "    \"A+\": 1,\n",
    "    \"َA++\": 2,\n",
    "    \"A+++\": 3,\n",
    "    \"A\": 0,\n",
    "    \"+A\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/764273650.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['energy_rating'] = dw['energy_rating'].replace(enery_rating_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "energy_rating\n",
       "2.0    29\n",
       "3.0     8\n",
       "1.0     3\n",
       "0.0     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw['energy_rating'] = dw['energy_rating'].replace(enery_rating_mapping)\n",
    "dw['energy_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/360741406.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dw['height'][dw['height'].isin([845, 850,846])] = dw['height'][dw['height'].isin([845, 850])] / 10\n",
      "/tmp/ipykernel_73601/360741406.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dw['height'][dw['height'].isin([845, 850,846])] = dw['height'][dw['height'].isin([845, 850])] / 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([85. ,  nan, 84.5, 84.6, 86. , 80. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw['height'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in dw.height]\n",
    "dw['height'] = dw['height'].astype(float)\n",
    "dw['height'][dw['height'].isin([845, 850,846])] = dw['height'][dw['height'].isin([845, 850])] / 10\n",
    "dw.height.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/2108558889.py:3: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  dw['depth'][dw['depth'].isin([600, 610,598])] = dw['depth'][dw['depth'].isin([600, 610,598])] / 10\n",
      "/tmp/ipykernel_73601/2108558889.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dw['depth'][dw['depth'].isin([600, 610,598])] = dw['depth'][dw['depth'].isin([600, 610,598])] / 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([60. ,  nan, 61. , 59.8, 66. , 55. ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw['depth'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in dw.depth]\n",
    "dw['depth'] = dw['depth'].astype(float)\n",
    "dw['depth'][dw['depth'].isin([600, 610,598])] = dw['depth'][dw['depth'].isin([600, 610,598])] / 10\n",
    "dw.depth.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/4140601651.py:33: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['weight'] = dw['weight'].replace(weights_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "weight\n",
       "46.0    8\n",
       "43.0    5\n",
       "62.0    4\n",
       "44.0    4\n",
       "52.0    4\n",
       "51.0    3\n",
       "47.5    3\n",
       "51.6    2\n",
       "48.0    2\n",
       "45.0    2\n",
       "50.0    2\n",
       "56.4    2\n",
       "51.5    2\n",
       "54.9    2\n",
       "54.2    1\n",
       "57.4    1\n",
       "61.0    1\n",
       "54.0    1\n",
       "56.9    1\n",
       "59.0    1\n",
       "8.0     1\n",
       "53.0    1\n",
       "43.5    1\n",
       "55.0    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_mapping = {\n",
    "    None: \"\",\n",
    "    '51.6 کیلوگرم': 51.6,\n",
    "    '54.2 کیلوگرم': 54.2,\n",
    "    '57.4 کیلوگرم':57.4,\n",
    "    '51.5 کیلوگرم': 51.5,\n",
    "    54.9: 54.9,\n",
    "    '54 کیلوگرم': 54,\n",
    "    '61 کیلوگرم': 61,\n",
    "    '62 کیلوگرم':62,\n",
    "    56.9: 56.9,\n",
    "    '56.4 کیلوگرم': 56.4,\n",
    "    '46000 گرم': 46,\n",
    "    '59 گرم': 59,\n",
    "    '50000 گرم': 50,\n",
    "    '43000 گرم': 43,\n",
    "    '51 کیلوگرم': 51,\n",
    "    '43 کیلوگرم': 43,\n",
    "    '8 گرم': 8,\n",
    "    '43 گرم': 43,\n",
    "    '46 گرم': 46,\n",
    "    '5200 گرم': 52,\n",
    "    '44 کیلوگرم': 44,\n",
    "    '45000 گرم': 45,\n",
    "    '53 گرم': 53,\n",
    "    '47500 گرم': 47.5,\n",
    "    '47.5 کیلوگرم': 47.5,\n",
    "    '48 کیلوگرم': 48,\n",
    "    '43.5 کیلوگرم': 43.5,\n",
    "    '55 گرم': 55,\n",
    "    None:np.nan\n",
    "}\n",
    "dw['weight'] = dw['weight'].replace(weights_mapping)\n",
    "dw['weight'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['60', None, '600 میلی متر', '600 میلیمتر', '600', '60 سانتی متر',\n",
       "       '59.8', '598 میلی متر', '59.8 سانتی\\u200c متر', '59.8 سانتی متر'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/2909240873.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['width'] = dw['width'].replace(width_mapping)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "width\n",
       "60.0    65\n",
       "59.8    11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width_mapping = {\n",
    "    '60': 60,\n",
    "    None: np.nan,\n",
    "    '600 میلی متر': 60,\n",
    "    '600 میلیمتر': 60,\n",
    "    '600': 60,\n",
    "    '60 سانتی متر': 60,\n",
    "    '59.8': 59.8,\n",
    "    '598 میلی متر': 59.8,\n",
    "    '59.8 سانتی\\u200c متر': 59.8,\n",
    "    '59.8 سانتی متر': 59.8\n",
    "}\n",
    "dw['width'] = dw['width'].replace(width_mapping)\n",
    "dw['width'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['سیستم قطع خودکار', '✅', 'دارد', None, 'قفل کودک', 'ندارد',\n",
       "       'سیستم هوشمند کنترل نشتی آب'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.safety_system.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/2732683827.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['safety_system'] = dw['safety_system'].replace(safety_system_mapping)\n"
     ]
    }
   ],
   "source": [
    "safety_system_mapping = {\n",
    "    'قفل کودک': 1,\n",
    "    'سیستم قطع خودکار': 1,\n",
    "    'سیستم ضد نشتی': 1,\n",
    "    'سیستم جلوگیری از نشت آب ظرفشویی': 1,\n",
    "    'ندارد': 0,\n",
    "    'دارد': 0,\n",
    "    'None': 0,\n",
    "    None: 0,\n",
    "    '✅':1,\n",
    "    'سیستم هوشمند کنترل نشتی آب':1,\n",
    "}\n",
    "\n",
    "dw['safety_system'] = dw['safety_system'].replace(safety_system_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['سه عدد', None, '3', '3 سبد', 2.0, 'دو عدد', '2 سبد', '2 عدد',\n",
       "       '1 عدد', '3 عدد', 'سه سبد', 'یک عدد'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.number_of_baskets.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/3614765347.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['number_of_baskets'] = dw['number_of_baskets'].replace(number_of_baskets_mapping)\n"
     ]
    }
   ],
   "source": [
    "number_of_baskets_mapping = {\n",
    "    'سه عدد': 3,\n",
    "    'دو عدد': 2,\n",
    "    'یک عدد': 1,\n",
    "    '3 سبد': 3,\n",
    "    '2 سبد': 2,\n",
    "    '1 سبد': 1,\n",
    "    'سه سبد': 3,\n",
    "    'دو سبد': 2,\n",
    "    'یک سبد': 1,\n",
    "    '3 عدد': 3,\n",
    "    '2 عدد': 2,\n",
    "    '1 عدد': 1,\n",
    "    '3': 3,\n",
    "    2.0:2,\n",
    "    None: np.nan\n",
    "}\n",
    "dw['number_of_baskets'] = dw['number_of_baskets'].replace(number_of_baskets_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['14', None, '13 نفر', '13 نفره', '6 نفره', '14 نفره', '15', '16',\n",
       "       '12', '12 نفره 144 پارچه', '12 نفره, 144 پارچه', '13', '15 نفره',\n",
       "       '13 لیتر', '28 فوت'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.capacity.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_mapping = {\n",
    "    \"14\": ['14', '14 نفره', '14 نفره'],\n",
    "    \"13\": ['13 نفر', '13 نفره', '13'],\n",
    "    \"6\": ['6 نفره'],\n",
    "    \"15\": ['15', '15 نفره'],\n",
    "    \"16\": ['16'],\n",
    "    \"12\": ['12', '12 نفره 144 پارچه', '12 نفره, 144 پارچه'],\n",
    "    np.nan: ['13 لیتر', '28 فوت'],\n",
    "}\n",
    "\n",
    "# Reverse the mapping to create a dictionary where the keys are current values and the values are target values\n",
    "reverse_capacity_mapping = {v: k for k, values in capacity_mapping.items() for v in values}\n",
    "\n",
    "# Apply the reverse mapping to the 'capacity' column\n",
    "dw['capacity'] = dw['capacity'].map(lambda x: reverse_capacity_mapping.get(x, np.nan))  # if not found, use np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level_mapping = {\n",
    "    None: np.nan,\n",
    "    44.0: 44,\n",
    "    '44 dB': 44,\n",
    "    42.0: 42,\n",
    "    '47': 47,\n",
    "    '43': 43,\n",
    "    '49 دسی\\u200cبل': 49,\n",
    "    '49 دسی بل': 49,\n",
    "    '47db': 47,\n",
    "    '49': 49,\n",
    "    '47 دسیبل': 47,\n",
    "    '47 دسی بل': 47,\n",
    "    '44 دسی\\u200cبل': 44,\n",
    "    '44 دسی بل': 44,\n",
    "    '42 دسی بل': 42,\n",
    "    '45': 45\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'noise_level' column\n",
    "dw['noise_level'] = dw['noise_level'].map(lambda x: noise_level_mapping.get(x, np.nan))  # if not found, use np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noise_level\n",
       "49.0    21\n",
       "47.0    13\n",
       "44.0    11\n",
       "42.0     4\n",
       "43.0     1\n",
       "45.0     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw['noise_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/3360294006.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['average_water_consumption_per_wash'] = dw['average_water_consumption_per_wash'].replace(average_water_consumption_mapping)\n"
     ]
    }
   ],
   "source": [
    "average_water_consumption_mapping = {\n",
    "    None: np.nan,\n",
    "    9.5: 9.5,\n",
    "    10.4: 10.4,\n",
    "    '10': 10,\n",
    "    '11 لیتر': 11,\n",
    "    '10 لیتر': 10,\n",
    "    '13 لیتر': 13,\n",
    "    '11': 11,\n",
    "    '13': 13,\n",
    "    '12 لیتر': 12,\n",
    "    '8 لیتر Wash شستشو - 8 لیتر آبکشی Rinse': 8,\n",
    "}\n",
    "dw['average_water_consumption_per_wash'] = dw['average_water_consumption_per_wash'].replace(average_water_consumption_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/2143079004.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dw['number_of_washing_programs'] = dw['number_of_washing_programs'].replace(number_of_washing_programs_mapping)\n"
     ]
    }
   ],
   "source": [
    "number_of_washing_programs_mapping = {\n",
    "    '7 برنامه': 7,\n",
    "    None: np.nan,\n",
    "    '6 برنامه': 6,\n",
    "    '8 برنامه': 8,\n",
    "    '8': 8,\n",
    "    '10': 10,\n",
    "    '5': 5,\n",
    "    '6 برنامه تخصصی شست‌وشو': 6,\n",
    "    '6': 6,\n",
    "    '7': 7,\n",
    "    '8 تا اصلی و 2 تا تکمیلی': 10,\n",
    "    '5 برنامه اصلی': 5,\n",
    "    '8+3': 11,\n",
    "    '10 برنامه': 10,\n",
    "    '12': 12\n",
    "}\n",
    "dw['number_of_washing_programs'] = dw['number_of_washing_programs'].replace(number_of_washing_programs_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.number_of_washing_programs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.price.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44800000. 39000000. 37600000. 42000000. 55500000. 51000000. 45000000.\n",
      " 16899000. 39500000.       nan 42400000. 61500000. 61000000. 63000000.\n",
      " 72400000. 60500000. 70000000. 71500000. 27000000. 38888000. 25400000.\n",
      " 29545000. 25000000. 36729000. 29350000. 31200000. 35000000. 29700000.\n",
      " 34500000. 31625000. 32900000. 28000000. 29800000. 30000000. 37630000.\n",
      " 24499000. 35124000. 28450000. 27500000. 28800000. 25500000. 33500000.\n",
      " 27100000. 31998300. 27900000. 28700000. 32700000. 29444000. 32000000.\n",
      " 39777000.]\n"
     ]
    }
   ],
   "source": [
    "dw['price'] = dw['price'].str.replace('تومان', '').str.replace('٫', '').str.replace(',', '').str.strip()\n",
    "dw['price'] = dw['price'].replace('ناموجود', np.nan)  # Replace 'ناموجود' with NaN\n",
    "dw['price'] = pd.to_numeric(dw['price'], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "\n",
    "print(dw['price'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand\n",
       "BOSCH       24\n",
       "PAKSHOMA    16\n",
       "GPLUS       15\n",
       "DAEWOO      13\n",
       "SNOWA        6\n",
       "X.VISION     6\n",
       "CANDY        4\n",
       "SAM          3\n",
       "ZEROWATT     3\n",
       "CORAL        1\n",
       "EMERSUN      1\n",
       "KENWOOD      1\n",
       "HIMALIA      1\n",
       "SAMSUNG      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract brand from the model_code by splitting at the hyphen and taking the first part\n",
    "dw['brand'] = dw['model_code'].str.split('-').str[0]\n",
    "dw['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.drop(['average_water_consumption_per_wash','noise_level','dishwasher_type'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model_code', 'price', 'height', 'depth', 'width', 'safety_system',\n",
       "       'number_of_baskets', 'capacity', 'weight', 'number_of_washing_programs',\n",
       "       'energy_rating', 'brand'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw['energy_rating'] = dw['energy_rating'].fillna(dw['energy_rating'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw['price'] = dw.groupby(['brand'])['price'].transform(lambda x: x.fillna(x.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 12)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.dropna(subset=['height','depth','width','capacity','weight'], how='all' , inplace=True)\n",
    "dw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['height', 'depth', 'width', 'weight']:\n",
    "    dw[col] = dw.groupby(['brand'])[col].transform(lambda x: x.fillna(x.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "height",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "depth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "width",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "capacity",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "weight",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "01245bf8-f43e-4b2a-b6b9-480e2668b582",
       "rows": [
        [
         "0",
         "85.0",
         "60.0",
         "60.0",
         "14",
         "55.30714285714284"
        ],
        [
         "4",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "51.6"
        ],
        [
         "6",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "51.6"
        ],
        [
         "7",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "54.2"
        ],
        [
         "8",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "57.4"
        ],
        [
         "9",
         "84.60714285714286",
         "60.0",
         "60.0",
         "13",
         "55.30714285714284"
        ],
        [
         "10",
         "84.60714285714286",
         "60.0",
         "60.0",
         "6",
         "55.30714285714284"
        ],
        [
         "11",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "51.5"
        ],
        [
         "12",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "51.5"
        ],
        [
         "13",
         "85.0",
         "60.0",
         "60.0",
         "14",
         "55.30714285714284"
        ],
        [
         "15",
         "84.60714285714286",
         "60.0",
         "60.0",
         null,
         "54.9"
        ],
        [
         "16",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "54.0"
        ],
        [
         "17",
         "84.60714285714286",
         "60.0",
         "60.0",
         null,
         "54.9"
        ],
        [
         "18",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "61.0"
        ],
        [
         "19",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "62.0"
        ],
        [
         "20",
         "85.0",
         "60.0",
         "60.0",
         "14",
         "55.30714285714284"
        ],
        [
         "21",
         "84.60714285714286",
         "60.0",
         "60.0",
         null,
         "56.9"
        ],
        [
         "22",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "56.4"
        ],
        [
         "23",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "56.4"
        ],
        [
         "24",
         "85.0",
         "60.0",
         "60.0",
         "15",
         "46.0"
        ],
        [
         "25",
         "85.0",
         "60.0",
         "60.0",
         "16",
         "59.0"
        ],
        [
         "26",
         "85.0",
         "60.0",
         "60.0",
         "15",
         "50.0"
        ],
        [
         "27",
         "85.0",
         "60.0",
         "60.0",
         "15",
         "50.0"
        ],
        [
         "28",
         "85.0",
         null,
         "60.0",
         "14",
         null
        ],
        [
         "29",
         "84.6",
         "60.0",
         "60.0",
         "12",
         "43.0"
        ],
        [
         "30",
         "84.77142857142859",
         "60.0",
         "60.0",
         "12",
         "46.42857142857143"
        ],
        [
         "32",
         "84.77142857142859",
         "60.0",
         "60.0",
         "12",
         "51.0"
        ],
        [
         "33",
         "84.77142857142859",
         "60.0",
         "60.0",
         "12",
         "51.0"
        ],
        [
         "34",
         "85.0",
         "60.0",
         "60.0",
         "15",
         "46.42857142857143"
        ],
        [
         "35",
         "85.0",
         "60.0",
         "60.0",
         "15",
         "46.42857142857143"
        ],
        [
         "36",
         "85.0",
         "60.0",
         "60.0",
         "14",
         "46.42857142857143"
        ],
        [
         "38",
         "84.77142857142859",
         "60.0",
         "60.0",
         "12",
         "51.0"
        ],
        [
         "39",
         "84.6",
         "60.0",
         "60.0",
         null,
         "43.0"
        ],
        [
         "40",
         "84.6",
         "60.0",
         "60.0",
         null,
         "43.0"
        ],
        [
         "41",
         "84.6",
         "60.0",
         "60.0",
         null,
         "43.0"
        ],
        [
         "42",
         "85.0",
         "60.0",
         "60.0",
         "14",
         "8.0"
        ],
        [
         "43",
         "84.5",
         "60.0",
         "59.8",
         "14",
         "62.0"
        ],
        [
         "44",
         "84.5",
         "60.0",
         "59.8",
         "14",
         "62.0"
        ],
        [
         "45",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "62.0"
        ],
        [
         "46",
         "84.5",
         "60.0",
         "60.0",
         "13",
         "62.0"
        ],
        [
         "47",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "62.0"
        ],
        [
         "48",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "62.0"
        ],
        [
         "49",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "62.0"
        ],
        [
         "50",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "62.0"
        ],
        [
         "51",
         "84.5",
         "60.0",
         "59.8",
         "15",
         "62.0"
        ],
        [
         "52",
         "84.5",
         "61.0",
         "59.8",
         "15",
         "62.0"
        ],
        [
         "53",
         "84.5",
         "61.0",
         "59.8",
         "15",
         "62.0"
        ],
        [
         "54",
         "84.5",
         "60.0",
         "59.8",
         "15",
         "62.0"
        ],
        [
         "55",
         "84.5",
         "60.0",
         "59.8",
         "15",
         "62.0"
        ],
        [
         "56",
         "84.5",
         "60.0",
         "60.0",
         "14",
         "62.0"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 87
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "      <th>depth</th>\n",
       "      <th>width</th>\n",
       "      <th>capacity</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>14</td>\n",
       "      <td>55.307143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13</td>\n",
       "      <td>51.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>84.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13</td>\n",
       "      <td>51.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13</td>\n",
       "      <td>54.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>13</td>\n",
       "      <td>57.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>85.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>14</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>59.8</td>\n",
       "      <td>14</td>\n",
       "      <td>43.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>84.5</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>16</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    height  depth  width capacity     weight\n",
       "0     85.0   60.0   60.0       14  55.307143\n",
       "4     84.5   60.0   60.0       13  51.600000\n",
       "6     84.5   60.0   60.0       13  51.600000\n",
       "7     84.5   60.0   60.0       13  54.200000\n",
       "8     84.5   60.0   60.0       13  57.400000\n",
       "..     ...    ...    ...      ...        ...\n",
       "90    85.0   59.8   60.0       14  48.000000\n",
       "91    85.0   60.0   59.8       14  43.500000\n",
       "92    84.5   60.0   60.0      NaN  46.000000\n",
       "93    85.0   60.0   60.0      NaN  50.500000\n",
       "94    85.0   60.0   60.0       16  55.000000\n",
       "\n",
       "[87 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw[['height','depth','width','capacity','weight']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73601/3218371624.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dw['number_of_baskets'].fillna(dw['number_of_baskets'].mode()[0], inplace=True)\n",
      "/tmp/ipykernel_73601/3218371624.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dw['number_of_washing_programs'].fillna(dw['number_of_washing_programs'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "dw['number_of_baskets'].fillna(dw['number_of_baskets'].mode()[0], inplace=True)\n",
    "dw['number_of_washing_programs'].fillna(dw['number_of_washing_programs'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw['capacity'] = dw.groupby(['height','depth','width'])['capacity'].transform(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.to_csv('dish_washer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp = dw.copy()\n",
    "dw_cp.drop(['model_code'], axis=1, inplace=True)\n",
    "dw_cp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp = pd.get_dummies(dw_cp, columns=['brand'], prefix='brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = dw_cp.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Correlation Heatmap of sbs_cp\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "scaler =StandardScaler()\n",
    "df_scaled = scaler.fit_transform(dw_cp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding number of cluster for kmeans with elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertia)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "dw['cluster'] = kmeans.fit_predict(df_scaled)\n",
    "# internal Evaluation Metrics\n",
    "score = silhouette_score(df_scaled, dw['cluster'])  # X = dataset, labels = cluster assignments\n",
    "print(f\"Silhouette Score: {score:.3f}\") #Closer to 1 → Well-defined clusters\n",
    "\n",
    "dbi = davies_bouldin_score(df_scaled, dw['cluster'])\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, dw['cluster'])\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n",
    "\n",
    "# Add the cluster labels to the original dataframe\n",
    "print(dw[['model_code', 'cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 3D for visualization\n",
    "pca = PCA(n_components=2,svd_solver='full')\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for the reduced features and cluster labels\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n",
    "df_pca_df['cluster'] = dw['cluster']\n",
    "\n",
    "# 3D Scatter Plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Scatter plot with cluster colors\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DBSCAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding min_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k in [5,10,15,20]:  # Test different min_samples values\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(df_scaled)\n",
    "    distances, indices = nbrs.kneighbors(df_scaled)\n",
    "    \n",
    "    sorted_distances = np.sort(distances[:, k-1])  # k-th nearest neighbor\n",
    "    plt.plot(sorted_distances, label=f'k={k}')\n",
    "\n",
    "plt.xlabel(\"Data Points Sorted\")\n",
    "plt.ylabel(\"k-Nearest Neighbor Distance\")\n",
    "plt.title(\"KNN Distance Plot for Different min_samples\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=3 ,min_samples=5)  # Adjust eps & min_samples as needed\n",
    "dw['cluster_dbscan'] = dbscan.fit_predict(df_scaled)  # Assign cluster labels\n",
    "\n",
    "score = silhouette_score(df_scaled, dw['cluster_dbscan'])  # X = dataset, labels = cluster assignments\n",
    "print(f\"Silhouette Score: {score:.3f}\") #Closer to 1 → Well-defined clusters\n",
    "\n",
    "dbi = davies_bouldin_score(df_scaled, dw['cluster_dbscan'])\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, dw['cluster_dbscan'])\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbs['best_dbscan']=best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = dw['cluster_dbscan']  # Add DBSCAN cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw.cluster_dbscan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# for k in range(2, 10):  # Try different k values\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     labels = kmeans.fit_predict(df_scaled)\n",
    "#     score = silhouette_score(df_scaled, labels)\n",
    "#     print(f'k={k}, Silhouette Score={score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_scores = []\n",
    "aic_scores = []\n",
    "n_components_range = range(1, 10)\n",
    "\n",
    "for n in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n, covariance_type='spherical', random_state=42)\n",
    "    gmm.fit(df_scaled)\n",
    "    bic_scores.append(gmm.bic(df_scaled))\n",
    "    aic_scores.append(gmm.aic(df_scaled))\n",
    "\n",
    "# Plot BIC and AIC scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(n_components_range, bic_scores, label='BIC', marker='o')\n",
    "plt.plot(n_components_range, aic_scores, label='AIC', marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.title('Optimal Number of Clusters using BIC & AIC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# Fit GMM model\n",
    "gmm = GaussianMixture(n_components=8, covariance_type='tied', random_state=42)\n",
    "gmm.fit(df_scaled)\n",
    "\n",
    "# Predict cluster labels\n",
    "clusters = gmm.predict(df_scaled)\n",
    "\n",
    "# Silhouette Score (Higher is better)\n",
    "sil_score = silhouette_score(df_scaled, clusters)\n",
    "print(f'Silhouette Score: {sil_score:.4f}')\n",
    "dbi = davies_bouldin_score(df_scaled, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n",
    "\n",
    "# Plot clustered data\n",
    "plt.scatter(df_scaled[:, 0], df_scaled[:, 1], c=clusters, cmap='viridis', s=15)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='x', s=100, label='Cluster Centers')\n",
    "plt.title(\"GMM Clustering Results\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = clusters # Add GMM cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability of each point belonging to clusters\n",
    "probs = gmm.predict_proba(df_scaled)\n",
    "print(\"Cluster Probabilities (First 5 points):\\n\", probs[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "best_score = -1\n",
    "feature_indices = list(range(10))  # 11 features\n",
    "# Try all combinations of 5 features out of 11\n",
    "for feature_subset in combinations(feature_indices, 5):\n",
    "    X_subset = df_scaled[:, feature_subset]  # Select features\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42).fit(X_subset)  # Assume 3 clusters\n",
    "    score = silhouette_score(X_subset, kmeans.labels_)  # Measure clustering quality\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_features = feature_subset\n",
    "\n",
    "print(\"Best Feature Indices:\", best_features)\n",
    "print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature_names = [dw_cp.columns[i] for i in best_features]\n",
    "best_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = kmeans.labels_ # Add GMM cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
