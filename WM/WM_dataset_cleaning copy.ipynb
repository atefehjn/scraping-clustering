{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from mapping_WM import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torob = pd.read_csv('WM_models_data_torob.csv')\n",
    "digikala = pd.read_csv('WM_models_data_digikala.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digikala.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torob.columns\n",
    "# with open('columns.txt', 'w', encoding='utf-8') as f:\n",
    "#     for i in a:\n",
    "#         f.write(i + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "def map_and_merge_columns(df, column_mappings):\n",
    "    df = df.copy()  # Avoid modifying the original DataFrame\n",
    "\n",
    "    for new_col, old_cols in column_mappings.items():\n",
    "        matching_cols = [col for col in old_cols if col in df.columns]\n",
    "        if matching_cols:\n",
    "            if len(matching_cols) > 1:\n",
    "                df[new_col] = df[matching_cols].apply(\n",
    "                    lambda row: next((x for x in row if pd.notna(x) and x != ''), None), axis=1\n",
    "                )\n",
    "                df.drop(columns=matching_cols, inplace=True)\n",
    "            else:\n",
    "                df.rename(columns={matching_cols[0]: new_col}, inplace=True)\n",
    "\n",
    "    # Select only existing columns\n",
    "    existing_cols = [col for col in column_mappings.keys() if col in df.columns]\n",
    "    return df[existing_cols]\n",
    "\n",
    "digikala_n = map_and_merge_columns(digikala, digikala_mapping)\n",
    "torob_n = map_and_merge_columns(torob, torob_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torob_n['model_code']= torob['model_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digikala_n.shape, torob_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torob_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm_concate = pd.concat([digikala_n, torob_n], ignore_index=True)\n",
    "\n",
    "# Group by 'model_code' and fill missing values with available ones\n",
    "wm = wm_concate.groupby(\"model_code\", as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wm[wm.duplicated(subset='model_code', keep=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "wm = wm.loc[:, wm.isnull().mean() < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_persian_to_english(text):\n",
    "    persian_digits = \"۰۱۲۳۴۵۶۷۸۹\"\n",
    "    english_digits = \"0123456789\"\n",
    "    trans_table = str.maketrans(persian_digits, english_digits)\n",
    "    return text.translate(trans_table)\n",
    "# Apply the conversion function to the entire DataFrame\n",
    "wm = wm.applymap(lambda x: convert_persian_to_english(str(x)) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [\n",
    "   \"رتبه انرژی\",\n",
    "        \"رتبه مصرف انرژی\",\n",
    "        \"برچسب مصرف انرژی\",\n",
    "        \"گرید انرژی\",\n",
    "        \"کلاس مصرف انرژی\",\n",
    "        \"نمودار مصرف انرژی\",\n",
    "        \"نمودار مصرفی انرژی\",\n",
    "        \"مصرف انرژی\",\n",
    "        \"میزان مصرف برق\",\n",
    "]\n",
    "for c in cc:\n",
    "    print(torob[c].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.energy_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enery_rating_mapping = {\n",
    "    \"+++A\": 3,\n",
    "    \"++A\": 2,\n",
    "    \"A++\": 2,\n",
    "    \"A+\": 1,\n",
    "    \"َA++\": 2,\n",
    "    \"A+++\": 3,\n",
    "    \"A\": 0,\n",
    "    \"+A\": 1,\n",
    "    'A+++ -35%': 3,\n",
    "    '196':np.nan,\n",
    "    'A+++ -55%': 3,\n",
    "    'A+++ – 30%': 3,\n",
    "    'A+++(-49)': 3,\n",
    "    'A+++ – 10%': 3,\n",
    "    'کم':np.nan,\n",
    "    'حداکثر توان: 2000 وات':np.nan,\n",
    "    '8+++': 3,\n",
    "    '+++A, ,, تا 30٪ صرفه جویی': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['energy_label'] = wm['energy_label'].replace(enery_rating_mapping)\n",
    "wm['energy_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['height'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in wm.height]\n",
    "wm['height'] = wm['height'].astype(float)\n",
    "wm.loc[wm['height'] > 800, 'height'] = wm.loc[wm['height'] > 800, 'height'] / 10\n",
    "\n",
    "wm.height.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['depth'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in wm.depth]\n",
    "wm['depth'] = wm['depth'].astype(float)\n",
    "wm.loc[wm['depth'] > 500, 'depth'] = wm.loc[wm['depth'] > 500, 'depth'] / 10\n",
    "wm.depth.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['width'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in wm.width]\n",
    "wm['width'] = wm['width'].astype(float)\n",
    "wm.loc[wm['width'] > 500, 'width'] = wm.loc[wm['width'] > 500, 'width'] / 10\n",
    "wm.width.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.drop(columns=['noise_level','display_status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_system_mapping = {\n",
    "    'قفل کودک': 1,\n",
    "    'سیستم خاموشی خودکار': 1,\n",
    "    'عیب یابی خودکار': 1,\n",
    "    'عیب\\u200cیابی خودکار':1,\n",
    "    'yes': 1,\n",
    "    'دارد': 1,\n",
    "    'دکمه توقف عملیات':1,\n",
    "    'سیستم قفل ایمنی':1,\n",
    "    'ندارد': 0,\n",
    "    'None': 0,\n",
    "    None: 0,\n",
    "}\n",
    "\n",
    "wm['safety_system'] = wm['safety_system'].replace(safety_system_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['motor_speed'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in wm.motor_speed]\n",
    "wm['motor_speed'] = wm['motor_speed'].astype(float)\n",
    "wm.motor_speed.unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['drum_capacity'] = [re.findall(r'\\d+\\.?\\d*', str(item))[0] if item is not None else None for item in wm.drum_capacity]\n",
    "wm['drum_capacity'] = wm['drum_capacity'].astype(float)\n",
    "wm['drum_capacity'][wm['drum_capacity'].isin([63])] = 9\n",
    "wm.drum_capacity.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "door_opening_mapping = {\n",
    "    \n",
    "    'به سمت چپ': 'left',\n",
    "    None:np.nan,\n",
    "    'از سمت راست':'left',\n",
    "    'به سمت پایین':'down',\n",
    "    'از راست به چپ':'left',\n",
    "    'به سمت چپ با زاویه 150 درجه':'left',\n",
    "    'جهت باز شدن به سمت چپ':'left',\n",
    "    'به سمت راست':'right',\n",
    "    'از سمت راست به چپ':'left',\n",
    "    'به سمت بالا':'up',\n",
    "    'از جلو':'left'\n",
    "}\n",
    "wm['door_opening_direction'] = wm['door_opening_direction'].replace(door_opening_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.motor_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motor_mapping = {\n",
    "    None: \"Unknown\",\n",
    "    \"universal\": \"Universal\",\n",
    "    \"یونیورسال\": \"Universal\",\n",
    "    \"موتور یونیورسال\": \"Universal\",\n",
    "    \"تسمه‌ای\": \"Belt Drive\",\n",
    "    \"تسمه ای\": \"Belt Drive\",\n",
    "    \"بدون تسمه\": \"Beltless\",\n",
    "    \"نیمه گیربکسی\": \"Semi-Gearbox\",\n",
    "    \"گیربکسی\": \"Gearbox\",\n",
    "    \"دایرکت درایو\": \"Direct Drive\",\n",
    "    \"موتور دایرکت درایو (بدون تسمه)\": \"Direct Drive\",\n",
    "    \"(Direct Drive)دایرکت درایو\": \"Direct Drive\",\n",
    "    \"دایرکت درایو (Direct Drive)\": \"Direct Drive\",\n",
    "    \"دایرکت داریور\": \"Direct Drive\",\n",
    "    \"دایرکت درایو(گیربکسی)\": \"Direct Drive Gearbox\",\n",
    "    \"موتور پیشرفته با تکنولوژیDirect Drive Inverter\": \"Direct Drive Inverter\",\n",
    "    \"اسمارت دایرکت درایو\": \"Smart Direct Drive\",\n",
    "    \"اینورتر BLDC\": \"BLDC Inverter\",\n",
    "    \"BLDC\": \"BLDC\",\n",
    "    \"bldc\": \"BLDC\",\n",
    "    \"BLDC Inverter\": \"BLDC Inverter\",\n",
    "    \"موتور BLDC\": \"BLDC\",\n",
    "    \"BLDC, تسمه ای\": \"BLDC Belt Drive\",\n",
    "    \"BLDC اینورتر\": \"BLDC Inverter\",\n",
    "    \"تسمه‌ای , BLDC\": \"BLDC Belt Drive\",\n",
    "    \"تسمه ای اینورتر (BLDC)\": \"BLDC Belt Drive\",\n",
    "    \"اینورتر (Inverter)\": \"Inverter\",\n",
    "    \"اینورتر\": \"Inverter\",\n",
    "    \"Inverter\": \"Inverter\",\n",
    "    \"اینورتر دیجیتال (Digital Inverter)\": \"Digital Inverter\",\n",
    "    \"دیجیتال اینورتر\": \"Digital Inverter\",\n",
    "    \"اینورتر Q-BLDC Drive\": \"Q-BLDC Drive\",\n",
    "    \"Q-BLDC Drive\": \"Q-BLDC Drive\",\n",
    "    \"اکو سایلنس درایو (EcoSilence Drive)\": \"EcoSilence Drive\",\n",
    "    \"EcoSilence Drive\": \"EcoSilence Drive\",\n",
    "    \"EcoSilence Drive : موتور پیشرفته و فوق العاده بی صدا با عمر طولانی مدت\": \"EcoSilence Drive\",\n",
    "    \"EcoSilence Drive: موتور فوق العاده ساکت با عمر طولانی\": \"EcoSilence Drive\",\n",
    "    \"موتور قدرتمند، کم مصرف و کم صدای EcoSilenceDrive\": \"EcoSilence Drive\",\n",
    "    \"EcoSilence Drive موتور قدرتمند، کم صدا با عمر طولانی\": \"EcoSilence Drive\",\n",
    "    \"موتور فوق العاده ساکت با عمر طولانی\": \"EcoSilence Drive\",\n",
    "    \"اکو سایلنس درایو (EcoSilence Drive) موتور فوق العاده ساکت با عمر طولانی\": \"EcoSilence Drive\",\n",
    "    \"EcoSilence Drive موتور فوق العاده ساکت با عمر طولانی\": \"EcoSilence Drive\",\n",
    "    \"BLDC: موتور فوق العاده ساکت با عمر طولانی\": \"BLDC\",\n",
    "    \"موتور تسمه ای یونیورسال\": \"Universal Belt Drive\",\n",
    "    \"Direct Drive Inverter\": \"Direct Drive Inverter\"\n",
    "}\n",
    "wm['motor_type'] = wm['motor_type'].replace(motor_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.tank_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drum_mapping = {\n",
    "    None: \"Unknown\",\n",
    "    \"درب از جلو\": \"Front Load\",\n",
    "    \"در از جلو\": \"Front Load\",\n",
    "    \"درب از بالا\": \"Top Load\",\n",
    "    \"استیل ضد زنگ\": \"Stainless Steel\",\n",
    "    \"استیل\": \"Stainless Steel\",\n",
    "    \"دیگ لانه زنبوری با تکنولوژی Water Magic Cube\": \"Honeycomb Drum (Water Magic Cube)\",\n",
    "    \"دیگ الماسه\": \"Diamond Drum\",\n",
    "    \"درام الماسه\": \"Diamond Drum\",\n",
    "    \"الماسه\": \"Diamond Drum\",\n",
    "    \"الماسه حباب ساز\": \"Bubble Diamond Drum\",\n",
    "    \"دیگ زمردی\": \"Emerald Drum\",\n",
    "    \"(Emerald) درام زمردی\": \"Emerald Drum\",\n",
    "    \"درام کلاستر (Cluster)\": \"Cluster Drum\",\n",
    "    \"کریستالی (Honeycomb)\": \"Crystal Honeycomb Drum\"\n",
    "}\n",
    "wm['tank_type'] = wm['tank_type'].replace(drum_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['price'] = wm['price'].str.replace('تومان', '').str.replace('٫', '').str.replace(',', '').str.strip()\n",
    "wm['price'] = wm['price'].replace('ناموجود', np.nan)  # Replace 'ناموجود' with NaN\n",
    "wm['price'] = pd.to_numeric(wm['price'], errors='coerce')  # Convert to numeric, coercing errors to NaN\n",
    "\n",
    "print(wm['price'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['brand'] = wm['model_code'].str.split('-').str[0]\n",
    "wm['brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [\n",
    "    'safety_system',\n",
    "    'motor_speed',\n",
    "    'door_opening_direction',\n",
    "    'energy_label'\n",
    "]\n",
    "for c in col:\n",
    "    wm[c] = wm[c].fillna(wm[c].mode()[0])\n",
    "# wm['energy_label'] = wm['energy_label'].fillna(wm['energy_label'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm['price'] = wm.groupby(['brand'])['price'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.dropna(subset=['height','depth','width','drum_capacity'], how='all' , inplace=True)\n",
    "wm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in ['height', 'depth', 'width','drum_capacity']:\n",
    "#     wm[col] = wm.groupby(['brand'])[col].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "for col in ['height', 'depth', 'width', 'drum_capacity']:\n",
    "    wm[col] = wm.groupby('brand')[col].transform(lambda x: x.fillna(round(x.mean(), 1)))\n",
    "        # wm[col] = wm.groupby(['brand'])[col].transform(lambda x: x.fillna(round(x.mean())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm[['height','depth','width','drum_capacity']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.dropna(inplace=True)\n",
    "wm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp = wm.copy()\n",
    "dw_cp.drop(['model_code'], axis=1, inplace=True)\n",
    "categorical_cols = dw_cp.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "dw_cp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp = pd.get_dummies(dw_cp, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = dw_cp.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(corr_matrix,annot=False, cmap='coolwarm', linewidths=0.2)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Correlation Heatmap of washing machine features\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper_tri.columns \n",
    "          if any((upper_tri[column] > threshold) | (upper_tri[column] < -threshold))]\n",
    "\n",
    "# Drop the highly correlated columns\n",
    "wm_cleaned = dw_cp.drop(columns=[col for col in to_drop if col in dw_cp.columns])\n",
    "\n",
    "print(f\"Dropped columns: {to_drop}\")\n",
    "# print(f\"Original shape: {wm.shape}\")\n",
    "print(f\"New shape: {wm_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "scaler =StandardScaler()\n",
    "df_scaled = scaler.fit_transform(wm_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding number of cluster for kmeans with elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(df_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertia)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "wm['cluster'] = kmeans.fit_predict(df_scaled)\n",
    "# internal Evaluation Metrics\n",
    "score = silhouette_score(df_scaled, wm['cluster'])  # X = dataset, labels = cluster assignments\n",
    "print(f\"Silhouette Score: {score:.3f}\") #Closer to 1 → Well-defined clusters\n",
    "\n",
    "dbi = davies_bouldin_score(df_scaled, wm['cluster'])\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, wm['cluster'])\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n",
    "\n",
    "# Add the cluster labels to the original dataframe\n",
    "print(wm[['model_code', 'cluster']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 3D for visualization\n",
    "pca = PCA(n_components=2,svd_solver='full')\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for the reduced features and cluster labels\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n",
    "df_pca_df['cluster'] = wm['cluster']\n",
    "\n",
    "# 3D Scatter Plot\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Scatter plot with cluster colors\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DBSCAN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding min_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k in [5,10,15,20]:  # Test different min_samples values\n",
    "    nbrs = NearestNeighbors(n_neighbors=k).fit(df_scaled)\n",
    "    distances, indices = nbrs.kneighbors(df_scaled)\n",
    "    \n",
    "    sorted_distances = np.sort(distances[:, k-1])  # k-th nearest neighbor\n",
    "    plt.plot(sorted_distances, label=f'k={k}')\n",
    "\n",
    "plt.xlabel(\"Data Points Sorted\")\n",
    "plt.ylabel(\"k-Nearest Neighbor Distance\")\n",
    "plt.title(\"KNN Distance Plot for Different min_samples\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=5 ,min_samples=5)  # Adjust eps & min_samples as needed\n",
    "wm['cluster_dbscan'] = dbscan.fit_predict(df_scaled)  # Assign cluster labels\n",
    "\n",
    "score = silhouette_score(df_scaled, wm['cluster_dbscan'])  # X = dataset, labels = cluster assignments\n",
    "print(f\"Silhouette Score: {score:.3f}\") #Closer to 1 → Well-defined clusters\n",
    "\n",
    "dbi = davies_bouldin_score(df_scaled, wm['cluster_dbscan'])\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, wm['cluster_dbscan'])\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbs['best_dbscan']=best_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = wm['cluster_dbscan']  # Add DBSCAN cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm.cluster_dbscan.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# for k in range(2, 10):  # Try different k values\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     labels = kmeans.fit_predict(df_scaled)\n",
    "#     score = silhouette_score(df_scaled, labels)\n",
    "#     print(f'k={k}, Silhouette Score={score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic_scores = []\n",
    "aic_scores = []\n",
    "n_components_range = range(1, 10)\n",
    "\n",
    "for n in n_components_range:\n",
    "    gmm = GaussianMixture(n_components=n, covariance_type='spherical', random_state=42)\n",
    "    gmm.fit(df_scaled)\n",
    "    bic_scores.append(gmm.bic(df_scaled))\n",
    "    aic_scores.append(gmm.aic(df_scaled))\n",
    "\n",
    "# Plot BIC and AIC scores\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(n_components_range, bic_scores, label='BIC', marker='o')\n",
    "plt.plot(n_components_range, aic_scores, label='AIC', marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.title('Optimal Number of Clusters using BIC & AIC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# Fit GMM model\n",
    "gmm = GaussianMixture(n_components=8, covariance_type='tied', random_state=42)\n",
    "gmm.fit(df_scaled)\n",
    "\n",
    "# Predict cluster labels\n",
    "clusters = gmm.predict(df_scaled)\n",
    "\n",
    "# Silhouette Score (Higher is better)\n",
    "sil_score = silhouette_score(df_scaled, clusters)\n",
    "print(f'Silhouette Score: {sil_score:.4f}')\n",
    "dbi = davies_bouldin_score(df_scaled, clusters)\n",
    "print(f\"Davies-Bouldin Index: {dbi:.3f}\") #Lower values indicate better clustering.\n",
    "\n",
    "ch = calinski_harabasz_score(df_scaled, clusters)\n",
    "print(f\"Calinski-Harabasz Index: {ch:.3f}\") #Higher values indicate better clustering.\n",
    "\n",
    "# Plot clustered data\n",
    "plt.scatter(df_scaled[:, 0], df_scaled[:, 1], c=clusters, cmap='viridis', s=15)\n",
    "plt.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='x', s=100, label='Cluster Centers')\n",
    "plt.title(\"GMM Clustering Results\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = clusters # Add GMM cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability of each point belonging to clusters\n",
    "probs = gmm.predict_proba(df_scaled)\n",
    "print(\"Cluster Probabilities (First 5 points):\\n\", probs[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "best_score = -1\n",
    "feature_indices = list(range(47))  # 11 features\n",
    "# Try all combinations of 5 features out of 11\n",
    "for feature_subset in combinations(feature_indices, 5):\n",
    "    X_subset = df_scaled[:, feature_subset]  # Select features\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42).fit(X_subset)  # Assume 3 clusters\n",
    "    score = silhouette_score(X_subset, kmeans.labels_)  # Measure clustering quality\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_features = feature_subset\n",
    "\n",
    "print(\"Best Feature Indices:\", best_features)\n",
    "print(\"Best Silhouette Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature_names = [dw_cp.columns[i] for i in best_features]\n",
    "best_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce to 3D for visualization using PCA\n",
    "pca = PCA(n_components=3)\n",
    "df_pca = pca.fit_transform(df_scaled)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_pca_df = pd.DataFrame(df_pca, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca_df['cluster'] = kmeans.labels_ # Add GMM cluster labels\n",
    "\n",
    "# Plot 3D Scatter\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot points, coloring them by cluster\n",
    "scatter = ax.scatter(df_pca_df['PC1'], df_pca_df['PC2'], df_pca_df['PC3'], \n",
    "                     c=df_pca_df['cluster'], cmap='viridis', alpha=0.8)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title('3D PCA of DBSCAN Clusters')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
